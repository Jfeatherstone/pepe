<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>pepe.tracking.Convolution API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pepe.tracking.Convolution</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np

import matplotlib.pyplot as plt

import numba
from scipy.fft import fft2, ifft2

from pepe.preprocess import circularMask

def circularKernelFind(singleChannelFrame, radius, fftPadding, debug=False, trimPadding=False):
    &#34;&#34;&#34;
    Calculate the convolution of a circular mask with an image,
    identifying likely locations of circles within the image. Adapted from
    method described in pages 64-68 of [1].

    To perform the convolution, we take the product of the fft of each the
    kernel and image, and then ifft the result to transform back to real space.
    The only peculiarity is that there can be some issues with the edges of image
    (especially if any circle centers are near the edge/off-screen) so we
    have to take care to properly pad the image and the kernel.

    Note that this method is not intended to be used as the front end for
    circle detection for real data. The method `pepe.tracking.convCircle()`
    makes use of this method to track circles, while also offering many more
    features. It is recommended to use that function unless there is a very
    specific purpose that it does not serve.

    Parameters
    ----------
    singleChannelFrame : np.ndarray[H,W]
        A single-channel image (grayscale or just one channel) within which we
        are looking to identify circular objects.

    radius : float
        The radius of circles to detect in the image.

    fftPadding : int
        The amount of padding to add to each side of the image, to prevent the
        fft from having issues. A good choice is usually 1.5 times the radius,
        though, less padding can be used if circles are not expected to often
        be located near the edges.

    debug : bool
        Whether or not to plot various quantities of the calculation for inspection
        at the end of the evaluation.

    trimPadding : bool
        Whether to trim the added padding to the image (True) or leave it in the final
        result (False). If trying to identify circles near the outskirts of the image,
        it is recommended to not trim the padding off, since the minimum of the function
        may be located there. In this case, the grid point at which the intensity is maximum
        will be offset from the actual center of that feature by an amount:

        `(-radius - fftPadding, -radius - fftPadding)`.

        If the padding is removed, the offset will simply be:

        `(-radius, -radius)`.

        This offset arises from the fact that the convolutional method expects the center of
        the kernel to be located at `(0, 0)`, but for any real image, we cannot have negative
        coordinates. We center the kernel in the top left corner of an image, but the center
        of the circle has to be at `(radius, radius)` for the entire shape to show up.
    Returns
    -------

    References
    ----------

    [1] Franklin, S.V., Shattuck, M.D. Handbook of Granular Materials (2016) Chapter 2:
        Experimental Techniques
    &#34;&#34;&#34;

    if singleChannelFrame.ndim &gt; 2:
        imageArr = np.mean(singleChannelFrame, axis=-1)
    else:
        imageArr = singleChannelFrame
    ## First, we calculate conv(I^2, W)

    # Pad the proper image
    paddedImageArr = np.zeros((imageArr.shape[0] + 2*fftPadding, imageArr.shape[1] + 2*fftPadding))
    # Note that the intensity is squared here, since we don&#39;t want to square the padding
    # later on
    #paddedImageArr[fftPadding:-fftPadding,fftPadding:-fftPadding] = imageArr**2
    paddedImageArr[fftPadding:-fftPadding,fftPadding:-fftPadding] = imageArr

    # Instead of just 0s in the padded area, we want to put in some real-ish values
    # We just use the tanh function to do this (though something like a sigmoid or (r)elu
    # would probably work fine too)
    #rowMat = (1 + np.tanh(3 * np.arange(-0.5, 0.5, paddedImageArr.shape[0])))/2
    #columnMat = (1 + np.tanh(3 * np.arange(-0.5, 0.5, paddedImageArr.shape[1])))/2 
    #Y, X = np.ogrid[:paddedImageArr.shape[0], :paddedImageArr.shape[1]]
    #paddingValues = .1/(1 + np.exp(3*(Y/paddedImageArr.shape[0] - .5)**2 + 3*(X/paddedImageArr.shape[1] - .5)**2))

    # Just some flat value
    #paddingValues = np.zeros_like(paddedImageArr) + .02

    # Now fill in the values
    #paddedImageArr[:fftPadding] = paddingValues[:fftPadding]
    #paddedImageArr[-fftPadding:] = paddingValues[-fftPadding:]
    #paddedImageArr[:,:fftPadding] = paddingValues[:,:fftPadding]
    #paddedImageArr[:,-fftPadding:] = paddingValues[:,-fftPadding:]

    # Top
    paddedImageArr[:fftPadding] += np.multiply.outer(2/(1 + np.exp(np.arange(0, 5, 5/fftPadding))), paddedImageArr[fftPadding])[::-1,:] 
    #paddedImageArr[:fftPadding] += np.multiply.outer(4/(1 + np.exp(np.arange(0, 3, 3/fftPadding))), np.repeat(np.mean(imageArr[0]), paddedImageArr.shape[1]))[::-1,:]
    # Bottom
    paddedImageArr[-fftPadding:] += np.multiply.outer(2/(1 + np.exp(np.arange(0, 5, 5/fftPadding))), paddedImageArr[-fftPadding-1])

    # Left
    paddedImageArr[:,:fftPadding] += np.multiply.outer(paddedImageArr[:,fftPadding], 2/(1 + np.exp(np.arange(0, 5, 5/fftPadding))))[:,::-1]
    # Right
    paddedImageArr[:,-fftPadding:] += np.multiply.outer(paddedImageArr[:,-fftPadding-1], 2/(1 + np.exp(np.arange(0, 5, 5/fftPadding))))


    #center = np.array([paddedImageArr.shape[0]/2, paddedImageArr.shape[1]/2])
    center = np.array([radius,radius])
    # To be able to properly convolute the kernel with the image, they have to
    # be the same size, so just just put our kernel into an array the same size
    # as the image (though most entries will just be zero)
    kernelArr = circularMask(paddedImageArr.shape, center, radius)[:,:,0].astype(np.float64)

    # First convolutional term (and clip out the padding)
    #convTerm1 = ifft2(fft2(paddedImageArr) * fft2(kernelArr))
    convTerm1 = ifft2(fft2(paddedImageArr**2) * fft2(kernelArr))

    if trimPadding:
        convTerm1 = convTerm1[fftPadding:-fftPadding,fftPadding:-fftPadding]

    # Second term is pretty easy since we choose our weight function to be our
    # particle mask
    # This value is explicitly:
    #convTerm2 = ifft2(fft2(kernelArr) * fft2(kernelArr**2))
    # But for our case, this is just the kernel itself (since it only has values of 0 and 1)
    convTerm2 = kernelArr
    #convTerm2 = 2 * ifft2(fft2(paddedImageArr) * fft2(kernelArr))

    if trimPadding:
        convTerm2 = convTerm2[fftPadding:-fftPadding,fftPadding:-fftPadding]

    # This is technically &lt;W I_p^2&gt;, but W = I_p which only have 0s or 1s
    normalizationTerm = np.sum(kernelArr)#[fftPadding:-fftPadding,fftPadding:-fftPadding])

    if trimPadding:
        normalizationTerm = np.sum(kernelAr[fftPadding:-fftPadding,fftPadding:-fftPadding])

    # Put everything together
    # Technically there is a +1 here, but that won&#39;t affect any minima, so we don&#39;t really care
    chiSqr = (convTerm1 - convTerm2) / normalizationTerm

    if debug:
        fig, ax = plt.subplots(1, 3, figsize=(10,4))

        ax[0].imshow(kernelArr)
        ax[0].set_title(&#39;Kernel&#39;)

        ax[1].imshow(paddedImageArr)
        ax[1].set_title(&#39;Padded image&#39;)

        ax[2].imshow(np.abs(chiSqr))
        ax[2].set_title(&#39;Real-space convolution (Norm)&#39;)

        fig.tight_layout()
        plt.show()


    return chiSqr


def kernelFind(image, kernel, fftPadding=100, debug=False):
    &#34;&#34;&#34;

    &#34;&#34;&#34;

    # Make sure we aren&#39;t passed multiple channels
    # If we do, we just average over all of the channels, making
    # the image grayscale (sorta)
    if image.ndim &gt; 2:
        imageArr = np.mean(image, axis=-1)
    else:
        imageArr = image

    if kernel.ndim &gt; 2:
        kernelArr = np.mean(kernel, axis=-1)
    else:
        kernelArr = kernel

    # Normalize
    imageArr /= np.max(imageArr)
    kernelArr /= np.max(kernelArr)

    # Pad the proper image
    paddedImageArr = np.zeros((imageArr.shape[0] + 2*fftPadding, imageArr.shape[1] + 2*fftPadding))
    paddedImageArr[fftPadding:-fftPadding,fftPadding:-fftPadding] = imageArr

    # Instead of just 0s in the padded area, we want to put in some real-ish values
    # We just use the tanh function to do this (though something like a sigmoid or (r)elu
    # would probably work fine too)
    #rowMat = (1 + np.tanh(3 * np.arange(-0.5, 0.5, paddedImageArr.shape[0])))/2
    #columnMat = (1 + np.tanh(3 * np.arange(-0.5, 0.5, paddedImageArr.shape[1])))/2
  
    Y, X = np.ogrid[:paddedImageArr.shape[0], :paddedImageArr.shape[1]]

    paddingValues = 1/(1 + np.exp(3*(Y/paddedImageArr.shape[0] - .5)**2 + 3*(X/paddedImageArr.shape[1] - .5)**2))

    #rowMat = 1/(1 + np.exp(np.arange(0, 1, paddedImageArr.shape[0])))
    #columnMat = 1/(1 + np.exp(np.arange(0, 1, paddedImageArr.shape[1])))

    #paddingValues = np.add.outer(rowMat, columnMat)

    # Now fill in the values
    paddedImageArr[:fftPadding] = paddingValues[:fftPadding]
    paddedImageArr[-fftPadding:] = paddingValues[-fftPadding:]
    paddedImageArr[:,:fftPadding] = paddingValues[:,:fftPadding]
    paddedImageArr[:,-fftPadding:] = paddingValues[:,-fftPadding:]

    # To be able to properly convolute the kernel with the image, they have to
    # be the same size, so just just put our kernel into an array the same size
    # as the image (though most entries will just be zero)
    resizedKernelArr = np.zeros_like(paddedImageArr)
    resizedKernelArr[0:kernelArr.shape[0],0:kernelArr.shape[1]] = kernelArr

    # Now we calculate the convolution of the image and kernel, which is just
    # the product of their fourier transforms
    convolutionFFTArr = fft2(paddedImageArr) * fft2(resizedKernelArr)

    # Now ifft back to real space, and then cut off the padding that we
    # introduced
    convolutionArr = ifft2(convolutionFFTArr)
    clippedConvolutionArr = convolutionArr[fftPadding:-fftPadding,fftPadding:-fftPadding]

    if debug:
        fig, ax = plt.subplots(1, 3, figsize=(10,4))

        ax[0].imshow(paddingValues)
        ax[0].set_title(&#39;FFT padding&#39;)

        ax[1].imshow(paddedImageArr)
        ax[1].set_title(&#39;Padded image&#39;)

        ax[2].imshow(np.real(clippedConvolutionArr))
        ax[2].set_title(&#39;Real-space convolution (Re)&#39;)

        fig.tight_layout()
        plt.show()

    return clippedConvolutionArr</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pepe.tracking.Convolution.circularKernelFind"><code class="name flex">
<span>def <span class="ident">circularKernelFind</span></span>(<span>singleChannelFrame, radius, fftPadding, debug=False, trimPadding=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the convolution of a circular mask with an image,
identifying likely locations of circles within the image. Adapted from
method described in pages 64-68 of [1].</p>
<p>To perform the convolution, we take the product of the fft of each the
kernel and image, and then ifft the result to transform back to real space.
The only peculiarity is that there can be some issues with the edges of image
(especially if any circle centers are near the edge/off-screen) so we
have to take care to properly pad the image and the kernel.</p>
<p>Note that this method is not intended to be used as the front end for
circle detection for real data. The method <code>pepe.tracking.convCircle()</code>
makes use of this method to track circles, while also offering many more
features. It is recommended to use that function unless there is a very
specific purpose that it does not serve.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>singleChannelFrame</code></strong> :&ensp;<code>np.ndarray[H,W]</code></dt>
<dd>A single-channel image (grayscale or just one channel) within which we
are looking to identify circular objects.</dd>
<dt><strong><code>radius</code></strong> :&ensp;<code>float</code></dt>
<dd>The radius of circles to detect in the image.</dd>
<dt><strong><code>fftPadding</code></strong> :&ensp;<code>int</code></dt>
<dd>The amount of padding to add to each side of the image, to prevent the
fft from having issues. A good choice is usually 1.5 times the radius,
though, less padding can be used if circles are not expected to often
be located near the edges.</dd>
<dt><strong><code>debug</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether or not to plot various quantities of the calculation for inspection
at the end of the evaluation.</dd>
<dt><strong><code>trimPadding</code></strong> :&ensp;<code>bool</code></dt>
<dd>
<p>Whether to trim the added padding to the image (True) or leave it in the final
result (False). If trying to identify circles near the outskirts of the image,
it is recommended to not trim the padding off, since the minimum of the function
may be located there. In this case, the grid point at which the intensity is maximum
will be offset from the actual center of that feature by an amount:</p>
<p><code>(-radius - fftPadding, -radius - fftPadding)</code>.</p>
<p>If the padding is removed, the offset will simply be:</p>
<p><code>(-radius, -radius)</code>.</p>
<p>This offset arises from the fact that the convolutional method expects the center of
the kernel to be located at <code>(0, 0)</code>, but for any real image, we cannot have negative
coordinates. We center the kernel in the top left corner of an image, but the center
of the circle has to be at <code>(radius, radius)</code> for the entire shape to show up.</p>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<h2 id="references">References</h2>
<p>[1] Franklin, S.V., Shattuck, M.D. Handbook of Granular Materials (2016) Chapter 2:
Experimental Techniques</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def circularKernelFind(singleChannelFrame, radius, fftPadding, debug=False, trimPadding=False):
    &#34;&#34;&#34;
    Calculate the convolution of a circular mask with an image,
    identifying likely locations of circles within the image. Adapted from
    method described in pages 64-68 of [1].

    To perform the convolution, we take the product of the fft of each the
    kernel and image, and then ifft the result to transform back to real space.
    The only peculiarity is that there can be some issues with the edges of image
    (especially if any circle centers are near the edge/off-screen) so we
    have to take care to properly pad the image and the kernel.

    Note that this method is not intended to be used as the front end for
    circle detection for real data. The method `pepe.tracking.convCircle()`
    makes use of this method to track circles, while also offering many more
    features. It is recommended to use that function unless there is a very
    specific purpose that it does not serve.

    Parameters
    ----------
    singleChannelFrame : np.ndarray[H,W]
        A single-channel image (grayscale or just one channel) within which we
        are looking to identify circular objects.

    radius : float
        The radius of circles to detect in the image.

    fftPadding : int
        The amount of padding to add to each side of the image, to prevent the
        fft from having issues. A good choice is usually 1.5 times the radius,
        though, less padding can be used if circles are not expected to often
        be located near the edges.

    debug : bool
        Whether or not to plot various quantities of the calculation for inspection
        at the end of the evaluation.

    trimPadding : bool
        Whether to trim the added padding to the image (True) or leave it in the final
        result (False). If trying to identify circles near the outskirts of the image,
        it is recommended to not trim the padding off, since the minimum of the function
        may be located there. In this case, the grid point at which the intensity is maximum
        will be offset from the actual center of that feature by an amount:

        `(-radius - fftPadding, -radius - fftPadding)`.

        If the padding is removed, the offset will simply be:

        `(-radius, -radius)`.

        This offset arises from the fact that the convolutional method expects the center of
        the kernel to be located at `(0, 0)`, but for any real image, we cannot have negative
        coordinates. We center the kernel in the top left corner of an image, but the center
        of the circle has to be at `(radius, radius)` for the entire shape to show up.
    Returns
    -------

    References
    ----------

    [1] Franklin, S.V., Shattuck, M.D. Handbook of Granular Materials (2016) Chapter 2:
        Experimental Techniques
    &#34;&#34;&#34;

    if singleChannelFrame.ndim &gt; 2:
        imageArr = np.mean(singleChannelFrame, axis=-1)
    else:
        imageArr = singleChannelFrame
    ## First, we calculate conv(I^2, W)

    # Pad the proper image
    paddedImageArr = np.zeros((imageArr.shape[0] + 2*fftPadding, imageArr.shape[1] + 2*fftPadding))
    # Note that the intensity is squared here, since we don&#39;t want to square the padding
    # later on
    #paddedImageArr[fftPadding:-fftPadding,fftPadding:-fftPadding] = imageArr**2
    paddedImageArr[fftPadding:-fftPadding,fftPadding:-fftPadding] = imageArr

    # Instead of just 0s in the padded area, we want to put in some real-ish values
    # We just use the tanh function to do this (though something like a sigmoid or (r)elu
    # would probably work fine too)
    #rowMat = (1 + np.tanh(3 * np.arange(-0.5, 0.5, paddedImageArr.shape[0])))/2
    #columnMat = (1 + np.tanh(3 * np.arange(-0.5, 0.5, paddedImageArr.shape[1])))/2 
    #Y, X = np.ogrid[:paddedImageArr.shape[0], :paddedImageArr.shape[1]]
    #paddingValues = .1/(1 + np.exp(3*(Y/paddedImageArr.shape[0] - .5)**2 + 3*(X/paddedImageArr.shape[1] - .5)**2))

    # Just some flat value
    #paddingValues = np.zeros_like(paddedImageArr) + .02

    # Now fill in the values
    #paddedImageArr[:fftPadding] = paddingValues[:fftPadding]
    #paddedImageArr[-fftPadding:] = paddingValues[-fftPadding:]
    #paddedImageArr[:,:fftPadding] = paddingValues[:,:fftPadding]
    #paddedImageArr[:,-fftPadding:] = paddingValues[:,-fftPadding:]

    # Top
    paddedImageArr[:fftPadding] += np.multiply.outer(2/(1 + np.exp(np.arange(0, 5, 5/fftPadding))), paddedImageArr[fftPadding])[::-1,:] 
    #paddedImageArr[:fftPadding] += np.multiply.outer(4/(1 + np.exp(np.arange(0, 3, 3/fftPadding))), np.repeat(np.mean(imageArr[0]), paddedImageArr.shape[1]))[::-1,:]
    # Bottom
    paddedImageArr[-fftPadding:] += np.multiply.outer(2/(1 + np.exp(np.arange(0, 5, 5/fftPadding))), paddedImageArr[-fftPadding-1])

    # Left
    paddedImageArr[:,:fftPadding] += np.multiply.outer(paddedImageArr[:,fftPadding], 2/(1 + np.exp(np.arange(0, 5, 5/fftPadding))))[:,::-1]
    # Right
    paddedImageArr[:,-fftPadding:] += np.multiply.outer(paddedImageArr[:,-fftPadding-1], 2/(1 + np.exp(np.arange(0, 5, 5/fftPadding))))


    #center = np.array([paddedImageArr.shape[0]/2, paddedImageArr.shape[1]/2])
    center = np.array([radius,radius])
    # To be able to properly convolute the kernel with the image, they have to
    # be the same size, so just just put our kernel into an array the same size
    # as the image (though most entries will just be zero)
    kernelArr = circularMask(paddedImageArr.shape, center, radius)[:,:,0].astype(np.float64)

    # First convolutional term (and clip out the padding)
    #convTerm1 = ifft2(fft2(paddedImageArr) * fft2(kernelArr))
    convTerm1 = ifft2(fft2(paddedImageArr**2) * fft2(kernelArr))

    if trimPadding:
        convTerm1 = convTerm1[fftPadding:-fftPadding,fftPadding:-fftPadding]

    # Second term is pretty easy since we choose our weight function to be our
    # particle mask
    # This value is explicitly:
    #convTerm2 = ifft2(fft2(kernelArr) * fft2(kernelArr**2))
    # But for our case, this is just the kernel itself (since it only has values of 0 and 1)
    convTerm2 = kernelArr
    #convTerm2 = 2 * ifft2(fft2(paddedImageArr) * fft2(kernelArr))

    if trimPadding:
        convTerm2 = convTerm2[fftPadding:-fftPadding,fftPadding:-fftPadding]

    # This is technically &lt;W I_p^2&gt;, but W = I_p which only have 0s or 1s
    normalizationTerm = np.sum(kernelArr)#[fftPadding:-fftPadding,fftPadding:-fftPadding])

    if trimPadding:
        normalizationTerm = np.sum(kernelAr[fftPadding:-fftPadding,fftPadding:-fftPadding])

    # Put everything together
    # Technically there is a +1 here, but that won&#39;t affect any minima, so we don&#39;t really care
    chiSqr = (convTerm1 - convTerm2) / normalizationTerm

    if debug:
        fig, ax = plt.subplots(1, 3, figsize=(10,4))

        ax[0].imshow(kernelArr)
        ax[0].set_title(&#39;Kernel&#39;)

        ax[1].imshow(paddedImageArr)
        ax[1].set_title(&#39;Padded image&#39;)

        ax[2].imshow(np.abs(chiSqr))
        ax[2].set_title(&#39;Real-space convolution (Norm)&#39;)

        fig.tight_layout()
        plt.show()


    return chiSqr</code></pre>
</details>
</dd>
<dt id="pepe.tracking.Convolution.kernelFind"><code class="name flex">
<span>def <span class="ident">kernelFind</span></span>(<span>image, kernel, fftPadding=100, debug=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kernelFind(image, kernel, fftPadding=100, debug=False):
    &#34;&#34;&#34;

    &#34;&#34;&#34;

    # Make sure we aren&#39;t passed multiple channels
    # If we do, we just average over all of the channels, making
    # the image grayscale (sorta)
    if image.ndim &gt; 2:
        imageArr = np.mean(image, axis=-1)
    else:
        imageArr = image

    if kernel.ndim &gt; 2:
        kernelArr = np.mean(kernel, axis=-1)
    else:
        kernelArr = kernel

    # Normalize
    imageArr /= np.max(imageArr)
    kernelArr /= np.max(kernelArr)

    # Pad the proper image
    paddedImageArr = np.zeros((imageArr.shape[0] + 2*fftPadding, imageArr.shape[1] + 2*fftPadding))
    paddedImageArr[fftPadding:-fftPadding,fftPadding:-fftPadding] = imageArr

    # Instead of just 0s in the padded area, we want to put in some real-ish values
    # We just use the tanh function to do this (though something like a sigmoid or (r)elu
    # would probably work fine too)
    #rowMat = (1 + np.tanh(3 * np.arange(-0.5, 0.5, paddedImageArr.shape[0])))/2
    #columnMat = (1 + np.tanh(3 * np.arange(-0.5, 0.5, paddedImageArr.shape[1])))/2
  
    Y, X = np.ogrid[:paddedImageArr.shape[0], :paddedImageArr.shape[1]]

    paddingValues = 1/(1 + np.exp(3*(Y/paddedImageArr.shape[0] - .5)**2 + 3*(X/paddedImageArr.shape[1] - .5)**2))

    #rowMat = 1/(1 + np.exp(np.arange(0, 1, paddedImageArr.shape[0])))
    #columnMat = 1/(1 + np.exp(np.arange(0, 1, paddedImageArr.shape[1])))

    #paddingValues = np.add.outer(rowMat, columnMat)

    # Now fill in the values
    paddedImageArr[:fftPadding] = paddingValues[:fftPadding]
    paddedImageArr[-fftPadding:] = paddingValues[-fftPadding:]
    paddedImageArr[:,:fftPadding] = paddingValues[:,:fftPadding]
    paddedImageArr[:,-fftPadding:] = paddingValues[:,-fftPadding:]

    # To be able to properly convolute the kernel with the image, they have to
    # be the same size, so just just put our kernel into an array the same size
    # as the image (though most entries will just be zero)
    resizedKernelArr = np.zeros_like(paddedImageArr)
    resizedKernelArr[0:kernelArr.shape[0],0:kernelArr.shape[1]] = kernelArr

    # Now we calculate the convolution of the image and kernel, which is just
    # the product of their fourier transforms
    convolutionFFTArr = fft2(paddedImageArr) * fft2(resizedKernelArr)

    # Now ifft back to real space, and then cut off the padding that we
    # introduced
    convolutionArr = ifft2(convolutionFFTArr)
    clippedConvolutionArr = convolutionArr[fftPadding:-fftPadding,fftPadding:-fftPadding]

    if debug:
        fig, ax = plt.subplots(1, 3, figsize=(10,4))

        ax[0].imshow(paddingValues)
        ax[0].set_title(&#39;FFT padding&#39;)

        ax[1].imshow(paddedImageArr)
        ax[1].set_title(&#39;Padded image&#39;)

        ax[2].imshow(np.real(clippedConvolutionArr))
        ax[2].set_title(&#39;Real-space convolution (Re)&#39;)

        fig.tight_layout()
        plt.show()

    return clippedConvolutionArr</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pepe.tracking" href="index.html">pepe.tracking</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="pepe.tracking.Convolution.circularKernelFind" href="#pepe.tracking.Convolution.circularKernelFind">circularKernelFind</a></code></li>
<li><code><a title="pepe.tracking.Convolution.kernelFind" href="#pepe.tracking.Convolution.kernelFind">kernelFind</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>